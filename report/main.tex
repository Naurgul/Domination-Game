\documentclass[conference]{IEEEtran}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
\title{A Rule-Based System for Controlling Agent Behaviour in the Domination Game}
\author{\IEEEauthorblockN{Koen Bonenkamp}
\IEEEauthorblockA{koen.bonenkamp@gmail.com}
\and
\IEEEauthorblockN{Vasileios Maragkos}
\IEEEauthorblockA{vasileios.maragkos@gmail.com}
\and
\IEEEauthorblockN{Mihai Morariu}
\IEEEauthorblockA{mihaimorariu@gmail.com}
\and
\IEEEauthorblockN{Nikolas Tzimoulis}
\IEEEauthorblockA{nikolaos@science.uva.nl}}

\maketitle

\begin{abstract}
%Designing a competitive multi-agent system that is able to take optimal actions in a partially observable environment is a challenging task. It requires learning a model of the environement and / or the opponent that can be used to plan a winning strategy for the competitive agent. These types of algorithms are usually known as "learning algorithms" and have proven successful in many real-world applications. Another type of algorithms that are commonly used in multi-agent systems are known as "rule-based algorithms". In the latter case, agents are pre-programmed to follow built-in strategies for different states they end up in. While they are easier to implement and typically run faster, they are not always guaranteed to yield an effective solution. In this paper, we present the solution  we developed for the final tournament of the \emph{Domination} game.
Designing a competitive squad-based multi-agent system that is capable of acting with reasonable competence in a partially observable environment is a challenging task. There are generally two schools of thought regarding how to tackle a problem like this. One approach is to codify the available knowledge of the domain into an algorithm. We call this approach rule-based, as a set of rules is one of the most straightforward and intuitive ways to achieve this. The other approach is to develop a system, dubbed a learning system, capable of extracting knowledge from the problem domain and then applying this knowledge during play. The \emph{Domination Game} is a simple team-based multiplayer game that can be used as a testing ground for both the aforementioned approaches. In this paper, we present a rule-based solution to the \emph{Domination Game} accompanied with experimental data about its performance. We also conduct an extensive discussion on the merits and shortfalls of both the rule-based and learning approach.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}

The \emph{Domination Game} \cite{domination} is a finite-horizon multi-agent game in which two teams of agents compete for the possession of three control points. A control point is captured the moment after a team has more agents standing on it than the other one and remains in that team's possession unless captured by the other team. Each agent has a limited range of visibility, making the world partially observable, but is allowed to communicate with their teammates. The map contains packs of ammo which an agent can collect and use to shoot at enemies. Whenever an agent is shot, it remains out of play for a specific duration and then respawns at a designated area, losing any ammo gathered. The map is procedurally generated at the beginning of each match. It is always symmetric with respect to the teams, but other than that, the control points, packs of ammo and walls that need to be navigated around are placed randomly, 

The teams start with an equal number of points. At each time step, a team's score is incremented or decremented based on the difference between the number of control points each team possesses. The game ends when either the time runs out, or one of the teams runs out of points. The team with the highest score is declared the winner.

%In this paper we describe the approach that we used for the \emph{Domination} game. We implemented both a rule-based agent and a learning agent, but used the rule-based one for the final tournament of the game. We present both of them and describe the issues we encountered with the learning agent and the reasons for which we decided to not submit the latter one.
%since, from our perspective, a learning algorithm would be time-consuming for this game and possibly computationally expensive, depending on how the state space is chosen. We also implemented a learning agent and we describe 
%From our experience from the previous Multi Agent Systems course, it takes a relatively large amount of time (~3000 episodes for a 7x7 grid) in order to learn an optimal policy by applying, for example, the Q-Learning algorithm.
%By the nature of the game, agents have a short amount of time in which they are required to take an action, otherwise they miss their turn. Therefore, an agent must be able to take decisions 

\subsection*{}

%The rest of the paper is organized as follows. Section \ref{sec:related_work} reviews the literature for multi-agent POMDPS and describes algorithms that are used to determine optimal policies for this type of problems. Section \ref{sec:reinforcement_learning} describes our learning agent and motivates our choice of submitting the rule-based agent for the final tournament. Section \ref{sec:methodology} describes the methodology we used for the tournament. Section \ref{sec:experimental_results} provides the results for the tests we have performed in order to assess the performance of our agent. Finally, section \ref{sec:conclusion} summarizes our findings and provides our ideas for further improvement.

The rest of the paper is organized as follows. Section \ref{sec:related_work} reviews the state of the practice of the game industry regarding the dilemma of using learning versus rule-based systems. Section \ref{sec:methodology} describes, in detail, our rule-based approach for solving the \emph{Domination Game}. Sections \ref{sec:experimental_setup} and \ref{sec:experimental_results} deal with the experiments we have performed in order to assess the performance of our system. Then, in Section \ref{sec:reinforcement_learning} we discuss some theory and ideas for a learning system, as well as provide some motivation for our focus on the rule-based system. Finally, Section \ref{sec:conclusion} summarizes our findings and provides our ideas for further improvement.

\section{Related Work}
\label{sec:related_work}

In the game industry, the question of whether AI systems need to learn by themselves or be told in advance how to act seems to be mostly settled in favour of the latter approach. The state of the practice in the industry is heavily geared towards planning, with a strong emphasis on efficiency \cite{gameAIworks}. This is justified, in the sense that a lot of planning techniques are of high time and space complexity and therefore do not fit well with the real-time constraints of the medium. 

With that in mind, it is no wonder that techniques like Behaviour Trees \cite{behaviourtrees}, which are little more than a pre-programmed variation of decision trees, have been particularly popular in the recent past. On the other hand, even the word ``learning'' is completely absent in the public discussions of industry participants, for example in the Game AI Conference in 2009 \cite{gameAIconf}. 

However, learning appears to introduce itself to the industry, mostly thanks to academic endeavours. For instance, the AIIDE 2010 Starcraft Competition \cite{starcraftcomp} was won by an agent developed in Berkeley \cite{overmind}. As part of its planning, the agent utilised potential fields whose parameters were tuned using reinforcement learning. 

Even though learning is not and probably cannot be used to solve comprehensive AI problems in the game industry for the time being, there is still a niche of sub-problems that can be effectively tackled using learning-based techniques, as long as the learning phase is part of the development process and does not happen in-game in real time.


\section{Methodology}
\label{sec:methodology}
In this section, we provide a detailed description of the techniques we used in our rule-based agent. We give an overview of the main challenges an agent faces throughout the game. We then analyze each of them separately and provide our solutions for tackling the respective problems. 

From our perspective, the result of a match hinges on the trade-off between two opposing tasks:
\begin{itemize}
\item Collecting ammo (and preventing it from falling into the hands of the enemy)
\item Capturing the control points (and preventing them from being captured by the enemy)
\end{itemize}
The problem of satisfying both tasks can be thought of as finding a way to share goals among agents. A straightforward way to achieve this is to assign a role to each agent on the team. The available roles are \emph{scouts} and \emph{troopers}. Scouts denote agents that are assigned the task to explore the map in order to find the locations of the ammo and to collect it. Troopers denote agents that are assigned the task of capturing and re-capturing control points. At this point, one might note that there is no explicit role for shooting enemies. In our system, this is considered a by-product of achieving our other goals.

%We now describe the two mentioned tasks that an agent must perform throughout the game and explain how our agents deal with them.

\subsection{Role assignment}

The roles are not considered completely separate and independent concepts; instead there is a flow for the agents from one role to the next. Since the primary objective is to capture control points, we expect opponents to congregate close at these locations, so it is important that our troopers are equipped with ammo to shoot at enemies within their range. For this reason, we consider agents that do not have ammo better suited to serve as scouts, while agents that do have ammo are always assigned the role of a trooper. In this way, agents alternate between the two roles: a scout after discovering ammo becomes a trooper, while a trooper after running out of ammo becomes a candidate for turning into a scout. 

The assignment of roles is conducted in a semi-independent manner. At each time step, the number of positions for the role of scout is calculated. We elected that this number should be proportional to the number of control points that are possessed by our team. We made this selection since we want our agents to focus on capturing control points first, especially if they are in the possession of only a few of them. We will explore further options for deciding this in Section \ref{sec:experimental_results}.

Then, if there are any newly open scout positions, any agent that has no ammo will fill them, on a first-come first-served basis. Closed scout positions are phased out progressively, as scouts find ammo and advance into being troopers.
 
\subsection{Collecting ammo}
\label{subsec:collecting_ammopacks}
Collecting ammo is essential in order to dominate the other team and win the match. In our system, the agents that are in charge of collecting it are the scouts. %But it is also important that the available ammopacks be split among teammates in order to increase the defense against the opponents. Once a scout discovers and collects an ammopack, it is assigned the role of a trooper and another agent who has no ammo is designated to become a scout.

An overview of scout behaviour is given in Algorithm \ref{alg:scout}. The reason why visible ammo locations are disregarded will be explained in Section \ref{sec:greedygoals}.If there is no convenient ammo location to go to and not all the ammo locations have been discovered, a scout begins to explore the map by moving towards the nodes on the mesh that no agent on the team has visited yet. When an ammo location is discovered, two entries are added to the agents' collective memory, since ammo locations are symmetric with respect to the $y$-axis. %A scout is provided with a list of the locations on the map where ammopacks were located up to that time step. Given this list, it decides towards which ammopack it should move based the distance between the current location and the locations of the previously seen ammopacks. 

That leaves the issue of ammo expectation: Since agents do not have full observability of the world, they cannot tell at each time step whether ammo is available at certain locations. The agents share knowledge regarding which ammo locations contain ammo and which ones do not. A timer $t$ is incremented for the locations which were observed not to have ammo in the past, while observing an ammo location that has ammo removes all doubt about when its ammo will be replenished\footnote{Therefore $t$ is set to its maximum value.}. The distance to each ammo location, for purposes of deciding which is the best one, $d_{final}$, is then increased inversely proportionally to its $t$-value: $d_{final} = d_{actual} + \frac{w_{ammo}}{t}$, where $d_{actual}$ is the actual distance and $w_{ammo}$ is a constant weight. The decision for the value of $w_{ammo}$ was the subject of experimentation, as will be seen in Section \ref{sec:experimental_results}.

\begin{algorithm}
\caption{Scout behaviour pseudocode}
\label{alg:scout}
\begin{algorithmic}
\STATE $ammo\_list \gets$ set of previously discovered ammo locations that are presently out of sight
\IF {$goal$ is not set from before}
	\STATE Sort $ammo\_list$ by proximity and ammo expectation
	\STATE $best\_loc \gets$ best element of $ammo\_list$ 
	\IF {the ammo expectation of $best\_loc$ is higher than some pre-defined threshold}
		\STATE $goal \gets best\_loc$
	\ENDIF
\ENDIF
\IF {$goal$ was not set above \AND not all ammo locations have been discovered}
	\STATE $goal \gets$ closest unexplored node
\ENDIF
\IF {$goal$ is still not set}
	\STATE Change role to trooper.
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{Capturing control points}
\label{subsec:capturing_control_points}
The objective of a trooper is capturing control points and making sure that the team maintains their possession. %The reason is that there is always the possibility that the opponents also be equipped with ammo and shoot at our agent, causing it to respawn and lose all the gathered ammopacks. For this reason, agents alternate between the scout / trooper roles whenever they run out of ammo / are equipped with ammo.

The behaviour of a trooper is defined by Algorithm \ref{alg:trooper}. A trooper tries to capture the closest control point from his current location and, if they spot an enemy approaching another control point, goes back to re-capture it. If the team is in the possession of all control points, troopers go \emph{spawn camping}, meaning that they engage in an assault meant to cage the enemies in their spawn area and prevent them from pursuing any objectives. 

\begin{algorithm}
\caption{Trooper behaviour pseudocode}
\label{alg:trooper}
\begin{algorithmic}
\IF {enemy is closer to one of our control points $cp$ than I am \AND there is no one on our team closer to $cp$ \AND my previous $goal$ is far away}
	\STATE $goal \gets cp$
\ENDIF
\IF {there is at least one control point we do not possess}
	\STATE $goal \gets$ closest un-captured control point
\ELSE \IF {I have ammo}
	\STATE $goal \gets$ enemy spawn area
\ELSE
	\STATE $goal \gets$ random control point
\ENDIF
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{Greedy goals}
\label{sec:greedygoals}

If agents were to strictly abide by their rigid role assignment, then there is a number of inefficiencies that would occur. It's not hard to imagine a scout passing next to an unprotected control point and not capturing it or, conversely, a trooper passing next to some ammo and deciding to leave it alone. For this reason, agents are allowed to act outside of their role responsibilities if some conditions are met. 

If an objective, ammo location that has ammo or an uncaptured control point, is within visible range of an agent and at the same time there is no other visible agent, friend or foe, that is closer to that objective, then the agent forgets about their previous goal and sets that objective as their goal. This explains why scouts don't explicitly consider ammo locations that are close: if they see one, they will go get it regardless of their role-defined goal.

\subsection{Low level improvements}

There is a number of small low-level improvements that were implemented. One of them is using path distance instead of Euclidean distance. When the distance to a position on the map is needed, we generate a path to that position and add the distances between the nodes in the path. That gives a more accurate metric when comparing distances. However, this is computationally costly so it not used when we do not expect that the additional accuracy will result in significantly better efficiency. 

Another is preventing friendly fire or wasting ammo: Agents are not allowed to shoot whenever there is an obstacle, such as a wall or a teammate, standing between them and their target enemy. Additionally, the agent is prevented from shooting if it is detected that their target is in the enemy spawn area.

Target selection is also a small but not insignificant detail. When an agent has a choice between multiple enemies to shoot at, they choose to shoot the one that would require them to make the smallest turn in orientation. 

Furthermore, turning and moving can get an agent farther from their target if the angle is not small enough. To counteract this, the speed is set to be proportional the absolute value of the turning angle that needs to be performed. 

In the same vein, we consider an agent stuck if they have not moved for a pre-defined number of time steps. An agent would detect whenever its location on the map had not changed for a few rounds in a row and change its orientation in order to escape. 

Finally, another feature that was added was goal sharing. Since multiple agents could be assigned the same goal throughout the game, we limited the maximum number of agents that could go for the same goal, to a pre-defined constant. The rest of the agents were assigned the task of capturing the control points.

%A problem that we faced in our experiments was that troopers were shooting at own teammates whenever the latter ones were being positioned between the former and enemies entering the visibility range. To overcome this, we implemented a feature that allows agents to shoot whenever there is no obstacle standing between them and the enemies (such as a wall), as well as a teammate. Another problem we faced was that agents from the same were occasionally colliding with each other and getting stuck when they were going for a goal. To overcome this, we implemented a feature that enables the agent to change position if it has not moved for a pre-defined number of stages.

\section{Experimental Setup}
\label{sec:experimental_setup}
In this section, we describe the experiments we performed in order to assess the performance of our agents. Each experiment consists of running 100 matches\footnote{Matches were run using the default game settings.} between our agents and an opponent. We have chosen three fixed opponents to use as a benchmark: one is the default agent while the others are taken from the agents submitted by other teams. We divide our experiments into two main categories: \emph{general progress} and \emph{parameter testing}. 

%\subsection{General Progress}
%\label{sub:agent_progress}
For the first category, we assessed the overall performance of our agent in various stages of development. %throughout the tournament in terms of average won games in order to see whether the features we included at each stage proved effective or not. We tested each of the three versions (for each stage of the tournament) we submitted against the default agent, as well as against the corresponding versions of the agents submitted by the two teams (team 1 and team 5) that ended up right ahead us in the final tournament.

%\subsection{Feature Progress}
%\label{sub:feature_progress}
For the second category, we assessed the performance effect of the values of individual parameters of the agent. We performed experiments for two parameters: 

The first parameter was the maximum number of scouts that are sent to collect ammo . We refer to this as the \emph{Scouts Experiment}. In our system this number is set to be proportional to the number of captured control points. For this experiment, we have tried setting it to be inversely proportional to the number of captured control points as well as constant and equal to half the team size.

%In our agent, the number of scouts is a function of the number of control points that are not possessed, given by the formula
%\begin{equation*}
%\mbox{max\_scouts} = \dfrac{\mbox{team\_size}}{\mbox{nr\_of\_not\_poss\_cps} + 1}
%\end{equation*}

%This means that whenever there is a larger number of control points that are not possessed, a smaller number of agents become scouts and, therefore, a larger number of agents are sent to take over the control points. In this experiment, we also tested the performance in terms of average number of won mathces against the opponents when:
%\begin{itemize}
%\item The number of scouts is equal to the number of troopers, namely
%\begin{equation*}
%\mbox{max\_scouts} = \dfrac{\mbox{team\_size}}{2}
%\end{equation*}
%\item The number of scouts depends of the number of control points that are possessed, namely 
%\begin{equation*}
%\mbox{max\_scouts} = \dfrac{\mbox{team\_size}}{\mbox{nr\_of\_poss\_cps} + 1}
%\end{equation*}
%\end{itemize}

For the second parameter, we tested various values for the parameter $w_{ammo}$, which controls the trade-off between ammo location distance and the availability of ammo there. We refer to this as the \emph{Ammopacks Experiment}. % For this reason, they need to estimate the respawn time and go for an ammopack only the moment they consider something is to be found at that specific location.

\section{Experimental Results}
\label{sec:experimental_results}
%In this section, we provide the results of our experiments.

\subsection{General progress}
The results from running the general progress experiment can be observed in Figure \ref{fig:prog}. It is obvious that while there was clear improvement between the first and the second version, the final version falls short of expectations. For comparison, the differences between the first version and the second one include the following features: preventing friendly fire and wasting ammo, path distance instead of Euclidean distance, full implementation of ammo expectation, conditions for exploration, target selection, greedy goals, troopers detect enemies close to possessed control points and go back to recapture. The differences between the second and the third version include getting unstuck after collisions and goal sharing. 

\begin{figure}[!h]
\centering
\includegraphics[width=\columnwidth]{progress}
\caption{Progress of our system during development. The development stages are depicted on the $x$-axis. Each line represents a different benchmark enemy system. The percentage of victories for our system is shown on the $y$-axis.}
\label{fig:prog}
\end{figure}

The interesting thing to note here is that during development each feature was tested against the previous versions of the agents and was only kept if there was an improvement. These last two features helped our system to defeat itself but were detrimental to its ability to defeat other teams. 

%While our agent constantly improved over the default agent, the features we implemented were not sufficient to make the agent always win against other opponents. Table \ref{table:1} shows the number of won matches (out of 100) against the default agent and the two teams that ended up right ahead us, for each version we submitted.
%\begin{table}[!h]
%\centering
%\begin{tabular}{|c|c|c|c|}
%\hline
%& Default agent & Team 1 & Team 5 \\
%\hline
%Tournament 1 & 73 & 23 & 2 \\
%\hline
%Tournament 2 & 99 & 94 & 20 \\
%\hline
%Tournament 3 & 100 & 82 & 19 \\
%\hline
%\end{tabular}
%\label{table:1}
%\caption{Number of wins of our agent against different opponents}
%\end{table}

%In tournament 2, there were some improvements regarding the behaviour of different classes of agents and goal selection, along with development of better cooperation and decision making methods.

%First of all there was a difference between the divisions of agents in the two classes, as described in the \emph{Experimental Setup} section. The "Greedy Goal" was introduced that integrated the ammopack hunting, disregarding the class if needed. Extra features were added in the behaviour of the two agent classes; the trooper would now return to a control point to defend it if there was an enemy heading that way, while a scout would stop being a scout if everything failed, meaning if there were no ammopacks that could be hunt down and nothing else to explore.

%Regarding the cooperation among the agents, there was an ammopack updating method available, which involved a list being shared between the agents. According to that list of "updated" ammopacks, the agents would avoid trying to visit the same ammopack in the same round. This saved a lot of unnecessary moves. Concerning the various decision making methods, one good example would be that the enemy that would be shot at, was selected after some more strict criteria. Main conditions also applied, like being in the right distance with no walls or fellows inside the line of fire, but there was a list generated with all those available to shoot at. From that list the best candidate would now be selected by the result of checking for whom the agent needed the least angle degrees for the turn action. Together with this minimum turn, there were many other functions computing the best distance from a target, used throughout the code.

%In tournament 3, collision detection was employed. A list containing a fixed number of self previous locations on the map was retained by each agent. An agent would detect whenever its location on the map had not changed for a couple of rounds and change its orientation in order to escape and reach the current goal. This feature did not improve performance significantly, but it allowed agents from the same team to escape whenever they were getting stuck into each other while going for the goals. 

%Another feature that was added was goal sharing. Since multiple agents could be assigned the same goal throughout the game, we limited the maximum number of agents that could go for the same goal, to a pre-defined constant. The rest of the agents were assigned the task of capturing the control points. This feature did not perform well in the final tournament, as the same version of the agent but without goal sharing proved to be more effective against the opponents.

\subsection{Parameter testing}
\subsubsection{Scouts Experiment}
Based on the match results versus the two benchmark teams\footnote{Performance against the default agent gave no insight after a point since all matches resulted in victory.}, employing fewer scouts to collect ammo whenever more control points were being in the possession of the agent, did not make any substantial difference. With the number of scouts being half of the team, no improvement could be observed either. All this is presented in Figure \ref{fig:scouts}. Note that switching from proportional to control points owned to inversely proportional, the results are slightly worse against one team while slightly better against the other. Perhaps a completely different scheme could give real improvement; for example, first sending the agents to collect all the available ammo and then assigning scout roles to agents as they run out of ammo.

\begin{figure}[!h]
\centering
\includegraphics[width=\columnwidth]{scouts}
\caption{Scout experiment. The various schemes for deciding the maximum number of scouts per benchmark opponent are shown on the $y$-axis. The result of the matches are shown in shades of grey on the $x$-axis: Black represents the percentage of victories for our system, white means the percentage of ties and gray signifies the percentage of victories for the benchmark.}
\label{fig:scouts}
\end{figure}

\subsubsection{Ammopacks Experiment}

The results can be seen in Figure \ref{fig:ammopacks}. Even big differences in the value of $w_{ammo}$ seem to make little difference to the result. This might imply that selecting the best ammo location is insignificant or a very small part of the problem in the grand scheme of things. Another possible interpretation is that the width of values tested is too small to show the effect of the variable to the results. 

\begin{figure}[!h]
\centering
\includegraphics[width=\columnwidth]{ammopacks}
\caption{Ammopacks experiment. The various values for $w_{ammo}$ per benchmark opponent are shown on the $y$-axis. The result of the matches are shown in shades of grey on the $x$-axis: Black represents the percentage of victories for our system, white means the percentage of ties and gray signifies the percentage of victories for the benchmark.}
\label{fig:ammopacks}
\end{figure}

\section{Reinforcement Learning}
\label{sec:reinforcement_learning}

Extensive research has been done over the last decades in order to come up with algorithms that are able to determine optimal solutions for decision-making problems. Some of the first algorithms were developed for single-agent MDP problems for which the agent has full knowledge of the current state and transitions. These include value iteration, policy iteration, dynamic programming and variants of them \cite{boutilier}.

In case of decision-making problems where the world is partially observable (POMDPs), it is much harder to determine an optimal solution. At every time step, the agent does not have full knowledge of the current state of the world. Problems with partial observability introduce a so-called \emph{belief space}, which represents a probability distribution over the state space that allows the agent to estimate the current state of the world. Therefore, finding optimal policies for POMDPs is a much more complex task than finding optimal policies for MDPs in the single-agent case. In the case of multi-agent problems, this complexity is even further increased. Some of the algorithms that determine an optimal policy through reinforcement learning include Q-Learning, Dyna, TD($\lambda$) and variants of them \cite{kaelbling}.

%For this reason, algorithms that approximate the solutions of POMDPs are preferred, although they come at the cost of a less accurate solution. In case of multi-agents, the methods that are used to learn a policy can be divided into two categories: \emph{supervised learning} and \emph{reinforcement learning}. In the former one, the learning algorithm uses a set of \emph{training examples} in order to learn an optimal policy. In the latter one, the agent needs to explore the environment in order to determine the model of the world, which is previously unknown. 

%In case of multi-agent systems with multiple learners, the environment is non-stationary, meaning that it is not sufficient to just train the agent before beginning the task (offline learning) and then apply the rules described by the optimal policy. Instead, the agent needs to adapt its policy at each stage during the game. This makes it more difficult to converge to a good strategy than it is in the case of single-agent systems. For this reason, we chose to implement a rule-based system in \emph{Domination}, since time is an important issue in the game and online adaption might exceed the maximum time alloted per agent at each stage. 

Apart from programming the agents in a rule-based manner, we experimented with reinforcement learning. In reinforcement learning, the actions that an agent takes in a particular situation (state) are learned instead of coded by hand. The agents learn, by linking state-action pairs to (future) rewards, for every state the agent should pick the action with the highest expected future cumulative reward.

%  During learning, agents should not always pick the action with the highest reward (greedy policy), instead with some probability they should explore. This way agents will converge to a globally optimal policy.

To apply reiforcement learning techniques to the domination game we would like the following to hold:
\begin{itemize}
\item We want to incorporate all that (environmental) information that is known to the agents into the state description (including an opponent model)
\item We want the agents to do the action selection in a centralised / coordinated way.%, and selection based on joint rewards
\end{itemize}

We are convinced that if we can sufficiently satisfy these two conditions, a team that learns efficiently will eventually outperform rule-based agents.

Unfortunately, this is an extremely hard problem. Modeling all this information and state-action pairs will easily become computationally intractable, as the first condition would make the state-action space enormous\footnote{Although the number of agents, control points, ammo locations and actions is rather small, a space that combines them all becomes enormous due to combinatorial explosion.} while the second condition would require an extended version of the Dec-POMDP framework. \cite{decpomdp}.

%In the papers by van Hasselt \cite{continuous} and Oliehoek \cite{decpomdp} both continuous state-action spaces and Dec-POMDPs are discussed. Both of them result in a computational complexity that is proven to be intractable, when computing a globally optimal policy.

As both conditions can't be fully met, some concessions need to be made. By making these concessions, we lose guarantees of arriving at a global optimum. The question that arises is if the local optimum that can be found results in a performance that can compete with rule-based agents. Later on we will discuss this question further.

By placing additional constraints on the model, we can make it smaller sacrificing objectivity in the process. For instance, a model containing enough information to learn solutions to the following sub-problems would be desirable:
\begin{itemize}
\item The trade-off between going for control points or ammo.
\item The trade-off between exploiting known ammo locations and exploring to discover new ones.
\item A team is almost guaranteed to win if it has ammo and it cages the enemy agents in at their spawn area
\end{itemize}

\subsection{State space}
\label{subsec:state_space}
If one tackles the problem by ignoring coordination completely, a suggestion for the state space for each agent could be:
\begin{itemize}
\item Logarithm\footnote{Note that by taking the logarithm we obtain more detailed information when we are closer to the locations and less detailed information when we are further away. The idea here is that the small fluctuations in distance are more important for small distances than bigger ones.} of distance to best\footnote{Based on distance, whether it's captured or not and other factors.} control point 
\item Logarithm of distance to best\footnote{Based on distance, expectation for ammo etc.} ammo location
\item Logarithm of distance to enemy spawn area
\item Number of control points possessed
\item Number of ammo locations that have been discovered so far
\item Amount of ammo the agent has in its possession 
\end{itemize}

%Every agent learned on its own (they should all learn the same policy so we allowed them to share the learned knowledge, see section about Q-learning).

This is already an enormous concession because the agent does not capture other agents in its state space, and there is no communication between the agents, advanced coordination can never be achieved by this modeling of the state space

%How did we select the 'best' points of interest (controlpoint and ammopack).
%We used selection techniques similar to our implementation of the rule-based agent, like distance to the POI, distance of other agents to the POI. For ammopack it also calculates the likelihood that ammo is available etc.
%The shooting action was also hard-coded and based on the rule-based agent techniques (such as determining if a teammate is inbetween you and the enemy before you shoot). If an agent can shoot an enemy player, it always picks the action shoot. Direction in which it is shooting is hard-coded aswell.

\subsection{Action space}
\label{subsec:action_space}
Continuing with the suggestion above, the action space may consist of the following super-actions:
\begin{itemize}
\item Move towards best control point.
\item Move towards best ammo location.
\item Move towards enemy spawn area.
\item Move towards best unexplored node.
\end{itemize}

\subsection{Reward signal}
\label{subsec:reward_signal}
Finally, to complete the model, we require a reward signal, something harder than may seem at first glance. One option would be to only give out reward at the end of the game\footnote{For example, 1 for winning, 0 for draw, -1 for losing.}. The other option is to give out reward at every time step.

A very interesting advantage of reinforcement learning techniques is that you can implement them without any bias. We, humans, are always biased when we learn, we have some initial thoughts about how to win this game (therefore we 'converge' very efficiently to a local optimum). But it is hard for humans to find a global optimum when this requires some out of the box thinking. By giving rewards at every time step we interfere with the ``tabula rasa'' way reinforcement learning is capable of. For example, if we give a small reward for shooting an enemy, we give the agent a bias for shooting and picking up ammo. This could sound like a good strategy but it actually is nothing more than a biased thought.

\subsection{Arms race}
\label{subsec:opponent}
In order for agents to learn, their enemy should not play too easy or too hard. Ideally, the enemy gets more difficult to beat during the learning process. In self-play the agent plays against itself. An interesting question to pose in that case is whether it is possible for both teams to share their Q-Value approximation and for both of them to push updates to it. Another idea is to have a set of progressively harder opponents, starting with the default agent and cumulating to a very advanced rule-based agent.

%To summarize our reinforcement learning model, we did basically three things:
%-let every agent learn/plan on it's own (decentralized)
%-create a compact discrete state and action space
%-implement some basic rule-based algorithms for shooting and ammopack/cp selection

\subsection{Limitations}
Some of the limitations of the model presented include:
\label{subsec:limitations}
\begin{itemize}
\item The shooting policy cannot be updated.
\item Opponents are not explicitly modeled or even kept track of.
\item Coordination is limited\footnote{Only indirectly by means of how the best control point and ammo locations is chosen.}.
\item Reward signals at every time step can introduce unnecessary bias.
\item History, a critical component of POMDPs, is not modeled at all.
\item Even the default agent could be too strong an opponent to start with.
\end{itemize}

\subsection{Experimentation}
\label{subsec:qlearning}
We implemented Q-learning on the model and experimented with different exploration and learning rates. We created one Q-value table for a team of agents, the six agents all updated the same table, this way we expected convergence to happen six times faster.
Due to the rule-based shooting and control point / ammo location selection, the agents were at least forced to always pick a target that made some sense. Unfortunately, the agents ended up switching actions every time step and therefore almost never reached a goal. One idea to alleviate this would be commitment sequences \cite{gameth}. A commitment sequence is a list of time slots for which an agent is committed to selecting always the same action. That would have the effect of minimising this undesirable behaviour and make learning possible.

After hours of computation time for learning we did not notice significant improvement. We analysed potential problems. A first intuition is that number of state-action pairs (82.944) is too large. We computed the average running time of a game, that is based on computation time per agent, this turned out to be approximately 15 seconds. The absolute lower bound of games required to try all state-action pairs with games of 600 time steps is 139. This lower bound will in fact never be reached as this requires every subsequent state-action pair to be unique for a sequence of 139 games. For future rewards to propagate to earlier state-action pairs many more games are required. Multiplying this large number of games with computation time clarified the fact that we didn't observe convergence. Although there are other factors that could be of influence as well; opponent is too hard, reward signal is not representative, state space is too elementary to allow intelligent reasoning etc.

We reconsidered the trade-off of defining a compact state space on the one hand and a very descriptive state space, to allow for more intelligent reasoning, on the other hand. As described in the Section \ref{subsec:limitations} the state description had already many limitations, making it even smaller would make it even less likely to allow for intelligent reasoning.

\section{Conclusion}
\label{sec:conclusion}
In this paper, we presented our approaches for the \emph{Domination} game. While a rule-based system was easier to implement and usually less computationally expensive in total, it proved difficult to find strategies that work effectively against all of the opponents. Implementing rules upon rules can have a detrimental effect to overall performance as rules collide with each other because the thinking behind each of them is made independently and without considering the bigger picture. Therefore, this incremental method of development is haphazard. 

On the other hand, a learning system, despite its potential, proved to be an infeasible solution due to hardware and time constraints. Focusing on smaller subproblems to be solved by reinforcement learning could be a feasible option\footnote{What would maybe be interesting for next year's course is to have more competition focus on reinforcement learning techniques. E.g. all teams get a rule-based agent that leaves some subproblems to be solved by reinforcement learning techniques. For the competition all teams should be constrained to using this 'template agent' and reinforcement learning techniques.}. Tuning parameters using constrained environments would allow for better results. However, taking a complex problem and hoping to reduce its complexity enough for it to become computationally tractable to solve using reinforcement learning is pointless. %Probably a better performance would be obtained by defining a set of rules that describe the behaviour of the agent and learning which one should be used in each situation, based on the previous experience. We plan to employ these changes in a future project.

In conclusion, the path for future projects with similar problem structure and requirements becomes clear: Use a top-down approach to design a rule-based framework for the agents. The ``atoms'' of this hierarchy, small problems in themselves, could be improved with the use of incremental software development techniques. Finally, learning techniques can be used as part of implementing any of these atoms, as long as the sub-problem to be solved is unintuitive to solve with creative thinking, requires continuity that cannot be captured using rules and is constrained enough so that techniques like reinforcement learning can be used. 

%Therefore we concluded that reinforcement learning techniques have interesting advantages and a lot of potential, but with our hardware and time constraints reinforcement learning did not have the potential to outperform ingenious developed rule-based agents.




\begin{thebibliography}{99}
\bibitem{domination} T. van den Berg and T. Doolan, \emph{Domination Game}, 2012, \url{https://github.com/noio/Domination-Game/}
\bibitem{boutilier} C. Boutilier, T. Dean, S. Hanks, \emph{Decision-Theoretic Planning: Structural Assumptions and Computational Leverage}, Journal of Artificial Intelligence Research, Vol. 11, 1-94, 1999.
\bibitem{kaelbling} L. P. Kaelbling, M. L. Littman, A. W. Moore, \emph{Reinforcement Learning: A Survey}, Journal of Artificial Intelligence Research, Vol. 4, 237-285, 1996
\bibitem{continuous} H. Hasselt, \emph{Reinforcement Learning in Continuous State and Action Spaces}, Reinforcement Learning: State of the Art, Chapter 7, Unpublished manuscript, 2012.
\bibitem{decpomdp} F.A. Oliehoek, \emph{Decentralized POMDPs}, Reinforcement Learning: State of the Art, Chapter 15, Unpublished manuscript, 2012.
\bibitem{gameth} A. Nowe´, P Vrancx, Y. De Hauwere, \emph{Game Theory and Multi-agent Reinforcement Learning}, Reinforcement Learning: State of the Art, Chapter 14, Unpublished manuscript, 2012.
\bibitem{gameAIworks} B. Hardwidge, \emph{How AI in Games Works}, bitGamer, 2009, \url{http://www.bit-tech.net/gaming/2009/03/05/how-ai-in-games-works/}.
\bibitem{behaviourtrees} R. Pillosu, \emph{Coordinating Agents with Behavior Trees}, Paris Game AI Conference, 2009, \url{http://files.aigamedev.com/coverage/GAIC09_CoordinatingAgents_RicardoPillosu.pdf}.
\bibitem{gameAIconf} A. J. Champandard, \emph{Paris Game AI Conference ’09: Highlights, Photos \& Slides}, 2009, \url{http://aigamedev.com/open/coverage/paris09-report/}.
\bibitem{starcraftcomp} Expressive Intelligence Studio at University of California, \emph{AIIDE 2010 Starcraft Competition}, 2010, \url{http://eis.ucsc.edu/StarCraftAICompetition}.
\bibitem{overmind} Dan Klein et al., \emph{The Berkeley Overmind Project}, 2010, \url{http://overmind.cs.berkeley.edu/}.
\end{thebibliography}

\end{document}


